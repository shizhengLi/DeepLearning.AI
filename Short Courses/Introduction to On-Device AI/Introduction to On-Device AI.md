# Introduction to On-Device AI



æœ¬æ–‡æ˜¯å­¦ä¹  [https://www.deeplearning.ai/short-courses/introduction-to-on-device-ai/](https://www.deeplearning.ai/short-courses/introduction-to-on-device-ai/)è¿™é—¨è¯¾çš„å­¦ä¹ ç¬”è®°ã€‚

![image-20240522133143885](./assets/image-20240522133143885.png)





## What youâ€™ll learn in this course

As AI moves beyond the cloud, on-device inference is rapidly expanding to smartphones, IoT devices, robots, AR/VR headsets, and more. Billions of mobile and other edge devices are ready to run optimized AI models. 

This course equips you with key skills to deploy AI on device:

- Explore how deploying models on device reduces latency, enhances efficiency, and preserves privacy.
- Go through key concepts of on-device deployment such as neural network graph capture, on-device compilation, and hardware acceleration.
- Convert pretrained models from PyTorch and TensorFlow for on-device compatibility.
- Deploy a real-time image segmentation model on device with just a few lines of code.
- Test your model performance and validate numerical accuracy when deploying to on-device environments
- Quantize and make your model up to 4x faster and 4x smaller for higher on-device performance.
- See a demonstration of the steps for integrating the model into a functioning Android app.

Learn from Krishna Sridhar, Senior Director of Engineering at Qualcomm, who has played a pivotal role in deploying over 1,000 models on devices and, with his team, has created the infrastructure used by over 100,000 applications.

By learning these techniques, youâ€™ll be positioned to develop and deploy AI to billions of devices and optimize your complex models to run efficiently on the edge.



@[toc]

# Why on-device?



![image-20240522133839571](./assets/image-20240522133839571.png)

applicable use-cases



![image-20240522133941140](./assets/image-20240522133941140.png)



Why on-device



![image-20240522134019604](./assets/image-20240522134019604.png)

Device in-the-loop deployment



![image-20240522134101108](./assets/image-20240522134101108.png)

on-device generative AI

![image-20240522134127224](./assets/image-20240522134127224.png)



# L2: Deploying Segmentation Models On-Device

è®¾å¤‡ä¸ŠAIçš„åº”ç”¨

![image-20240522135113952](./assets/image-20240522135113952.png)



Image segmentation

![image-20240522135135655](./assets/image-20240522135135655.png)



types of image segmentation

![image-20240522135217774](./assets/image-20240522135217774.png)

image segmentationçš„åº”ç”¨

![image-20240522135234615](./assets/image-20240522135234615.png)

semantic segmentationæ¨¡å‹



![image-20240522135403504](./assets/image-20240522135403504.png)

FFNet

![image-20240522135434367](./assets/image-20240522135434367.png)



[FFNet Paper](https://arxiv.org/abs/2206.08236)



![image-20240522135508473](./assets/image-20240522135508473.png)





```py
from qai_hub_models.models.ffnet_40s import Model
from torchinfo import summary
# Load from pre-trained weights
model = Model.from_pretrained()
input_shape = (1, 3, 1024, 2048)
stats = summary(model, 
  input_size=input_shape, 
  col_names=["num_params", "mult_adds"]
)
print(stats)
```

Output

```py
Loading pretrained model state dict from /home/jovyan/.qaihm/models/ffnet/v1/ffnet40S/ffnet40S_dBBB_cityscapes_state_dict_quarts.pth
Initializing ffnnet40S_dBBB_mobile weights
==============================================================================================================
Layer (type:depth-idx)                                       Param #                   Mult-Adds
==============================================================================================================
FFNet40S                                                     --                        --
â”œâ”€FFNet: 1-1                                                 --                        --
â”‚    â””â”€ResNetS: 2-1                                          --                        --
â”‚    â”‚    â””â”€Conv2d: 3-1                                      864                       452,984,832
â”‚    â”‚    â””â”€BatchNorm2d: 3-2                                 64                        64
â”‚    â”‚    â””â”€ReLU: 3-3                                        --                        --
â”‚    â”‚    â””â”€Conv2d: 3-4                                      18,432                    2,415,919,104
â”‚    â”‚    â””â”€BatchNorm2d: 3-5                                 128                       128
â”‚    â”‚    â””â”€ReLU: 3-6                                        --                        --
â”‚    â”‚    â””â”€Sequential: 3-7                                  300,160                   9,797,895,296
â”‚    â”‚    â””â”€Sequential: 3-8                                  1,411,840                 11,542,727,424
â”‚    â”‚    â””â”€Sequential: 3-9                                  3,900,288                 7,977,571,200
â”‚    â”‚    â””â”€Sequential: 3-10                                 7,071,360                 3,617,592,960
â”‚    â””â”€FFNetUpHead: 2-2                                      --                        --
â”‚    â”‚    â””â”€Sequential: 3-11                                 1,208,147                 26,571,541,312
==============================================================================================================
Total params: 13,911,283
Trainable params: 13,911,283
Non-trainable params: 0
Total mult-adds (G): 62.38
==============================================================================================================
Input size (MB): 25.17
Forward/backward pass size (MB): 1269.30
Params size (MB): 55.65
Estimated Total Size (MB): 1350.11
==============================================================================================================
```

utils.py

```py
import os
from dotenv import load_dotenv, find_dotenv


def load_env():
    _ = load_dotenv(find_dotenv())

def get_ai_hub_api_token():
    load_env()
    ai_hub_api_token = os.getenv("AI_HUB_API_KEY")
    return ai_hub_api_token

```

## Exercise: Try another variant of FFNet



```py
# High resolution variants
from qai_hub_models.models.ffnet_40s import Model
#from qai_hub_models.models.ffnet_54s import Model
#from qai_hub_models.models.ffnet_78s import Model

# Low resolution variants
low_res_input_shape = (1, 3, 512, 1024)
#from qai_hub_models.models.ffnet_78s_lowres import Model
#from qai_hub_models.models.ffnet_122ns_lowres import Model

model = Model.from_pretrained()
stats = summary(model, 
  input_size=input_shape, # use low_res_input_shape for low_res models
  col_names=["num_params", "mult_adds"]
)
print(stats)
```

Output

```py
Loading pretrained model state dict from /home/jovyan/.qaihm/models/ffnet/v1/ffnet40S/ffnet40S_dBBB_cityscapes_state_dict_quarts.pth
Initializing ffnnet40S_dBBB_mobile weights
==============================================================================================================
Layer (type:depth-idx)                                       Param #                   Mult-Adds
==============================================================================================================
FFNet40S                                                     --                        --
â”œâ”€FFNet: 1-1                                                 --                        --
â”‚    â””â”€ResNetS: 2-1                                          --                        --
â”‚    â”‚    â””â”€Conv2d: 3-1                                      864                       452,984,832
â”‚    â”‚    â””â”€BatchNorm2d: 3-2                                 64                        64
â”‚    â”‚    â””â”€ReLU: 3-3                                        --                        --
â”‚    â”‚    â””â”€Conv2d: 3-4                                      18,432                    2,415,919,104
â”‚    â”‚    â””â”€BatchNorm2d: 3-5                                 128                       128
â”‚    â”‚    â””â”€ReLU: 3-6                                        --                        --
â”‚    â”‚    â””â”€Sequential: 3-7                                  300,160                   9,797,895,296
â”‚    â”‚    â””â”€Sequential: 3-8                                  1,411,840                 11,542,727,424
â”‚    â”‚    â””â”€Sequential: 3-9                                  3,900,288                 7,977,571,200
â”‚    â”‚    â””â”€Sequential: 3-10                                 7,071,360                 3,617,592,960
â”‚    â””â”€FFNetUpHead: 2-2                                      --                        --
â”‚    â”‚    â””â”€Sequential: 3-11                                 1,208,147                 26,571,541,312
==============================================================================================================
Total params: 13,911,283
Trainable params: 13,911,283
Non-trainable params: 0
Total mult-adds (G): 62.38
==============================================================================================================
Input size (MB): 25.17
Forward/backward pass size (MB): 1269.30
Params size (MB): 55.65
Estimated Total Size (MB): 1350.11
==============================================================================================================
```



## Setup AI Hub for device-in-the-loop deployment

```py
import qai_hub

from utils import get_ai_hub_api_token
ai_hub_api_token = get_ai_hub_api_token()

!qai-hub configure --api_token $ai_hub_api_token
```

Output

```py
qai-hub configuration saved to /home/jovyan/.qai_hub/client.ini
==================== /home/jovyan/.qai_hub/client.ini ====================
[api]
api_token = eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJhcHAiLCJzdWIiOiIxNzQ2MDYyIiwiYXVkIjoiV0VCIiwiaWF0IjoxNzE2MzU2MDYxLCJleHAiOjE3MTg5NDgwNjF9.b2yWxfQnX8bVMrncob3vCQX5-g4kduq84m5DlvYoU78
api_url = https://app.aihub.qualcomm.com
web_url = https://app.aihub.qualcomm.com
verbose = True
```



```py
%run -m qai_hub_models.models.ffnet_40s.demo
```

Output

![image-20240522134744625](./assets/image-20240522134744625.png)

## Run on a real smart phone!

```py
%run -m qai_hub_models.models.ffnet_40s.export -- --device "Samsung Galaxy S23"
```

Output

```py
    âœ… SUCCESS                          

------------------------------------------------------------
Performance results on-device for Ffnet_40S.
------------------------------------------------------------
Device                          : Samsung Galaxy S23 (13)
Runtime                         : TFLITE                 
Estimated inference time (ms)   : 23.3                   
Estimated peak memory usage (MB): [3, 5]                 
Total # Ops                     : 92                     
Compute Unit(s)                 : NPU (92 ops)           
------------------------------------------------------------
More details: https://app.aihub.qualcomm.com/jobs/jn5q228m5/

Waiting for inference job (j1glkknlp) completion. Type Ctrl+C to stop waiting at any time.
    âœ… SUCCESS                          
dataset-dd9pg5on9.h5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.22M/1.22M [00:00<00:00, 11.5MB/s]

Comparing on-device vs. local-cpu inference for Ffnet_40S.
+---------------+-------------------+--------+
| output_name   | shape             |   psnr |
+===============+===================+========+
| output_0      | (1, 19, 128, 256) |  62.96 |
+---------------+-------------------+--------+

- psnr: Peak Signal-to-Noise Ratio (PSNR). >30 dB is typically considered good.
```



## On Device Demo

```py
%run -m qai_hub_models.models.ffnet_40s.demo -- --device "Samsung Galaxy S23" --on-device
```

Output

![image-20240522135004548](./assets/image-20240522135004548.png)





# L3: Preparing for on-device deployment

On-device deployment key concepts

![image-20240522142132810](./assets/image-20240522142132810.png)



graph capture

![image-20240522142228745](./assets/image-20240522142228745.png)



## Capture trained model



```py
from qai_hub_models.models.ffnet_40s import Model as FFNet_40s

# Load from pre-trained weights
ffnet_40s = FFNet_40s.from_pretrained()

import torch
input_shape = (1, 3, 1024, 2048)
example_inputs = torch.rand(input_shape)

traced_model = torch.jit.trace(ffnet_40s, example_inputs)

traced_model
```



## Compile for device

![image-20240522142353912](./assets/image-20240522142353912.png)



```py
import qai_hub
import qai_hub_models

from utils import get_ai_hub_api_token
ai_hub_api_token = get_ai_hub_api_token()

!qai-hub configure --api_token $ai_hub_api_token
```

Output

```py
qai-hub configuration saved to /home/jovyan/.qai_hub/client.ini
==================== /home/jovyan/.qai_hub/client.ini ====================
[api]
api_token = eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJhcHAiLCJzdWIiOiIxNzQ2MDYyIiwiYXVkIjoiV0VCIiwiaWF0IjoxNzE2MzU2MDYxLCJleHAiOjE3MTg5NDgwNjF9.b2yWxfQnX8bVMrncob3vCQX5-g4kduq84m5DlvYoU78
api_url = https://app.aihub.qualcomm.com
web_url = https://app.aihub.qualcomm.com
verbose = True
```





```py
for device in qai_hub.get_devices():
    print(device.name)
```

Output

```py
Google Pixel 3
Google Pixel 3a
Google Pixel 3 XL
Google Pixel 4
Google Pixel 4
Google Pixel 4a
Google Pixel 5
Samsung Galaxy Tab S7
Samsung Galaxy Tab A8 (2021)
Samsung Galaxy Note 20 (Intl)
Samsung Galaxy S21
Samsung Galaxy S21+
Samsung Galaxy S21 Ultra
Xiaomi Redmi Note 10 5G
Google Pixel 3a XL
Google Pixel 4a
Google Pixel 5
Google Pixel 5a 5G
Google Pixel 6
Samsung Galaxy A53 5G
Samsung Galaxy A73 5G
RB3 Gen 2 (Proxy)
QCS6490 (Proxy)
RB5 (Proxy)
QCS8250 (Proxy)
QCS8550 (Proxy)
Samsung Galaxy S21
Samsung Galaxy S21 Ultra
Samsung Galaxy S22 Ultra 5G
Samsung Galaxy S22 5G
Samsung Galaxy S22+ 5G
Samsung Galaxy Tab S8
Xiaomi 12
Xiaomi 12 Pro
Google Pixel 6
Google Pixel 6a
Google Pixel 7
Google Pixel 7 Pro
Samsung Galaxy A14 5G
Samsung Galaxy S22 5G
QCS8450 (Proxy)
XR2 Gen 2 (Proxy)
Samsung Galaxy S23
Samsung Galaxy S23+
Samsung Galaxy S23 Ultra
Google Pixel 7
Google Pixel 8
Google Pixel 8 Pro
Samsung Galaxy S24
Samsung Galaxy S24 Ultra
Samsung Galaxy S24+
```





```py
device = qai_hub.Device("Samsung Galaxy S23")

# Compile for target device
compile_job = qai_hub.submit_compile_job(
    model=traced_model,                        # Traced PyTorch model
    input_specs={"image": input_shape},        # Input specification
    device=device,                             # Device
)
```





```py
# Download and save the target model for use on-device
target_model = compile_job.get_target_model()
```





## Exercise: Try different runtimes

Target runtime

![image-20240522142505463](./assets/image-20240522142505463.png)

TensorFlow Lite



![image-20240522142540278](./assets/image-20240522142540278.png)



```py
compile_options="--target_runtime tflite"                  # Uses TensorFlow Lite
compile_options="--target_runtime onnx"                    # Uses ONNX runtime
compile_options="--target_runtime qnn_lib_aarch64_android" # Runs with Qualcomm AI Engine

compile_job_expt = qai_hub.submit_compile_job(
    model=traced_model,                        # Traced PyTorch model
    input_specs={"image": input_shape},        # Input specification
    device=device,                             # Device
    options=compile_options,
)
```



Expore more compiler options <a href=https://app.aihub.qualcomm.com/docs/hub/compile_examples.html#compiling-pytorch-to-tflite> here</a>.



## On-Device Performance Profiling



```py
from qai_hub_models.utils.printing import print_profile_metrics_from_job

# Choose device
device = qai_hub.Device("Samsung Galaxy S23")

# Runs a performance profile on-device
profile_job = qai_hub.submit_profile_job(
    model=target_model,                       # Compiled model
    device=device,                            # Device
)

# Print summary
profile_data = profile_job.download_profile()
print_profile_metrics_from_job(profile_job, profile_data)
```

Output

```py
------------------------------------------------------------
Performance results on-device for Job_Jqp4Wxrlg_Optimized_Tflite.
------------------------------------------------------------
Device                          : Samsung Galaxy S23 (13)
Runtime                         : TFLITE                 
Estimated inference time (ms)   : 30.1                   
Estimated peak memory usage (MB): [0, 2]                 
Total # Ops                     : 94                     
Compute Unit(s)                 : NPU (94 ops)           
------------------------------------------------------------
```



## Exercise: Try different compute units

```py
profile_options="--compute_unit cpu"     # Use cpu 
profile_options="--compute_unit gpu"     # Use gpu (with cpu fallback) 
profile_options="--compute_unit npu"     # Use npu (with cpu fallback) 

# Runs a performance profile on-device
profile_job_expt = qai_hub.submit_profile_job(
    model=target_model,                     # Compiled model
    device=device,                          # Device
    options=profile_options,
)
```



## On-Device Inference



```py
torch_inputs = torch.Tensor(sample_inputs['image'][0])
torch_outputs = ffnet_40s(torch_inputs)
torch_outputs
```





```py
inference_job = qai_hub.submit_inference_job(
        model=target_model,          # Compiled model
        inputs=sample_inputs,        # Sample input
        device=device,               # Device
)
```



```py
ondevice_outputs = inference_job.download_output_data()
ondevice_outputs['output_0']
```

å¯¹æ¯”on-deviceå’Œlocal-cpuæ¨ç†



```py
from qai_hub_models.utils.printing import print_inference_metrics
print_inference_metrics(inference_job, ondevice_outputs, torch_outputs)
```

Output

```py
Comparing on-device vs. local-cpu inference for Job_Jqp4Wxrlg_Optimized_Tflite.
+---------------+----------------------------+--------+
| output_name   | shape                      |   psnr |
+===============+============================+========+
| output_0      | torch.Size([19, 128, 256]) |  62.96 |
+---------------+----------------------------+--------+

- psnr: Peak Signal-to-Noise Ratio (PSNR). >30 dB is typically considered good.
```



## Get ready for deployment!

```py
target_model = compile_job.get_target_model()
_ = target_model.download("FFNet_40s.tflite")
```

review

![image-20240522143413838](./assets/image-20240522143413838.png)

# L4: Quantizing Models

Why quantize



![image-20240522144036507](./assets/image-20240522144036507.png)



What is quantization

![image-20240522144116878](./assets/image-20240522144116878.png)

Scale and zero point

![image-20240522144230366](./assets/image-20240522144230366.png)

Types of quantization

weight quantization and activation quantization





![image-20240522144442558](./assets/image-20240522144442558.png)



Post Training Quantization (PTQ)

Quantization Aware Training (QAT)





![image-20240522144834184](./assets/image-20240522144834184.png)



4 steps in code

![image-20240522145122843](./assets/image-20240522145122843.png)

```py
from datasets import load_dataset

# Use input resolution of the network
input_shape = (1, 3, 1024, 2048)

# Load 100 RGB images of urban scenes 
dataset = load_dataset("UrbanSyn/UrbanSyn", 
                split="train", 
                data_files="rgb/*_00*.png")
dataset = dataset.train_test_split(1)

# Hold out for testing
calibration_dataset = dataset["train"]
test_dataset = dataset["test"]

calibration_dataset["image"][0]
```

Output

![image-20240522143623624](./assets/image-20240522143623624.png)



## Setup calibration/inference pipleline

```py
import torch
from torchvision import transforms

# Convert the PIL image above to Torch Tensor
preprocess = transforms.ToTensor()

# Get a sample image in the test dataset
test_sample_pil = test_dataset[0]["image"]
test_sample = preprocess(test_sample_pil).unsqueeze(0) 
print(test_sample)

```

Output

```py
tensor([[[[0.0941, 0.1020, 0.2941,  ..., 0.5176, 0.4784, 0.4510],
          [0.0980, 0.1059, 0.2000,  ..., 0.5137, 0.4902, 0.4745],
          [0.1098, 0.1294, 0.2275,  ..., 0.4980, 0.4863, 0.4980],
          ...,
          [0.4784, 0.5020, 0.5098,  ..., 0.5882, 0.5686, 0.5608],
          [0.4941, 0.5098, 0.5294,  ..., 0.5020, 0.5098, 0.4824],
          [0.4980, 0.5137, 0.5333,  ..., 0.4588, 0.4353, 0.4157]],

         [[0.1098, 0.1020, 0.2431,  ..., 0.5176, 0.4784, 0.4549],
          [0.1137, 0.1294, 0.1922,  ..., 0.5098, 0.4902, 0.4745],
          [0.1294, 0.1608, 0.2078,  ..., 0.4980, 0.4863, 0.4980],
          ...,
          [0.5059, 0.5176, 0.5255,  ..., 0.5647, 0.5333, 0.5294],
          [0.5137, 0.5333, 0.5451,  ..., 0.4745, 0.4745, 0.4431],
          [0.5176, 0.5373, 0.5569,  ..., 0.4275, 0.3922, 0.3804]],

         [[0.0824, 0.0784, 0.2353,  ..., 0.5294, 0.4824, 0.4510],
          [0.0824, 0.0784, 0.1647,  ..., 0.5216, 0.4980, 0.4863],
          [0.0667, 0.0902, 0.1843,  ..., 0.5059, 0.4941, 0.5176],
          ...,
          [0.5412, 0.5412, 0.5490,  ..., 0.5843, 0.5451, 0.5412],
          [0.5529, 0.5725, 0.5765,  ..., 0.4902, 0.4902, 0.4627],
          [0.5490, 0.5804, 0.6039,  ..., 0.4353, 0.4039, 0.3882]]]])
```



```py
import torch.nn.functional as F
import numpy as np
from PIL import Image

def postprocess(output_tensor, input_image_pil):

    # Upsample the output to the original size
    output_tensor_upsampled = F.interpolate(
        output_tensor, input_shape[2:], mode="bilinear",
    )

    # Get top predicted class and convert to numpy
    output_predictions = output_tensor_upsampled[0].argmax(0).byte().detach().numpy().astype(np.uint8)

    # Overlay over original image
    color_mask = Image.fromarray(output_predictions).convert("P")

    # Create an appropriate palette for the Cityscapes classes
    palette = [
        128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156,
        190, 153, 153, 153, 153, 153, 250, 170, 30, 220, 220, 0,
        107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60,
        255, 0, 0, 0, 0, 142, 0, 0, 70, 0, 60, 100, 0, 80, 100,
        0, 0, 230, 119, 11, 32]
    palette = palette + (256 * 3 - len(palette)) * [0]
    color_mask.putpalette(palette)
    out = Image.blend(input_image_pil, color_mask.convert("RGB"), 0.5)
    return out
```



## Setup model in floating point



```py
from qai_hub_models.models.ffnet_40s.model import FFNet40S
model = FFNet40S.from_pretrained().model.eval()

# Run sample output through the model
test_output_fp32 = model(test_sample)
test_output_fp32

postprocess(test_output_fp32, test_sample_pil)
```

Output

![image-20240522143748550](./assets/image-20240522143748550.png)

## Prepare Quantized Model



```py
from qai_hub_models.models._shared.ffnet_quantized.model import FFNET_AIMET_CONFIG
from aimet_torch.batch_norm_fold import fold_all_batch_norms
from aimet_torch.model_preparer import prepare_model
from aimet_torch.quantsim import QuantizationSimModel

# Prepare model for 8-bit quantization
fold_all_batch_norms(model, [input_shape])
model = prepare_model(model)

# Setup quantization simulator
quant_sim = QuantizationSimModel(
    model,
    quant_scheme="tf_enhanced",
    default_param_bw=8,              # Use bitwidth 8-bit
    default_output_bw=8,
    config_file=FFNET_AIMET_CONFIG,
    dummy_input=torch.rand(input_shape),
)
```



## Perform post training quantization



```py
size = 5  # Must be < 100

def pass_calibration_data(sim_model: torch.nn.Module, args):
    (dataset,) = args
    with torch.no_grad():
        for sample in dataset.select(range(size)):
            pil_image = sample["image"]
            input_batch = preprocess(pil_image).unsqueeze(0)

            # Feed sample through for calibration
            sim_model(input_batch)

# Run Post-Training Quantization (PTQ)
quant_sim.compute_encodings(pass_calibration_data, [calibration_dataset])
```



```py
test_output_int8 = quant_sim.model(test_sample)
postprocess(test_output_int8, test_sample_pil)
```

Output

![image-20240522144006624](./assets/image-20240522144006624.png)



## Run Quantized model on-device



```py
import qai_hub
import qai_hub_models

from utils import get_ai_hub_api_token
ai_hub_api_token = get_ai_hub_api_token()

!qai-hub configure --api_token $ai_hub_api_token
```





```py
%run -m qai_hub_models.models.ffnet_40s_quantized.export -- --device "Samsung Galaxy S23"
```

Output

```py
------------------------------------------------------------
Performance results on-device for Ffnet_40S_Quantized.
------------------------------------------------------------
Device                          : Samsung Galaxy S23 (13)
Runtime                         : TFLITE                 
Estimated inference time (ms)   : 6.4                    
Estimated peak memory usage (MB): [1, 9]                 
Total # Ops                     : 97                     
Compute Unit(s)                 : NPU (97 ops)           
------------------------------------------------------------
More details: https://app.aihub.qualcomm.com/jobs/jvgdvreeg/
dataset-dq9k16r52.h5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 770k/770k [00:00<00:00, 7.23MB/s]
Comparing on-device vs. local-cpu inference for Ffnet_40S_Quantized.
+---------------+-------------------+--------+
| output_name   | shape             |   psnr |
+===============+===================+========+
| output_0      | (1, 19, 128, 256) |  33.93 |
+---------------+-------------------+--------+

- psnr: Peak Signal-to-Noise Ratio (PSNR). >30 dB is typically considered good.
```



Impact of quantization

![image-20240522145827325](./assets/image-20240522145827325.png)

é—®é¢˜ï¼š Post-Training Quantization (PTQ)ä¸ºä»€ä¹ˆéœ€è¦calibration_datasetï¼Œcalibrationæ˜¯ä»€ä¹ˆ?



Post-Training Quantization (PTQ) éœ€è¦ calibration datasetï¼ˆæ ¡å‡†æ•°æ®é›†ï¼‰çš„åŸå› æ˜¯ä¸ºäº†ç¡®å®šæ¨¡å‹å„å±‚çš„é‡åŒ–å‚æ•°ï¼Œç‰¹åˆ«æ˜¯ scaleï¼ˆæ¯”ä¾‹ï¼‰å’Œ zero-pointï¼ˆé›¶ç‚¹ï¼‰ã€‚Calibration æ˜¯æŒ‡ä½¿ç”¨æ ¡å‡†æ•°æ®é›†æ¥ç»Ÿè®¡å’Œè®¡ç®—æ¨¡å‹çš„æ¿€æ´»å€¼å’Œæƒé‡çš„åˆ†å¸ƒï¼Œä»è€Œç¡®å®šæœ€ä½³çš„é‡åŒ–å‚æ•°ï¼Œä»¥åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å°½é‡å‡å°‘ç²¾åº¦æŸå¤±ã€‚



Calibration çš„é‡è¦æ€§

- **ç¡®å®šé‡åŒ–èŒƒå›´**ï¼šæµ®ç‚¹æ•°è½¬æ¢ä¸ºæ•´æ•°è¡¨ç¤ºæ—¶ï¼Œéœ€è¦ç¡®å®šé‡åŒ–èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œå¯¹äº8ä½é‡åŒ–ï¼Œæ¿€æ´»å€¼å’Œæƒé‡éœ€è¦è¢«æ˜ å°„åˆ°0åˆ°255çš„èŒƒå›´ã€‚Calibration æ•°æ®é›†å¸®åŠ©ç¡®å®šæ¯ä¸€å±‚çš„æµ®ç‚¹å€¼èŒƒå›´ï¼Œä»¥ä¾¿è¿›è¡Œå‡†ç¡®çš„æ˜ å°„ã€‚
- **å‡å°‘é‡åŒ–è¯¯å·®**ï¼šé€šè¿‡ç»Ÿè®¡æ ¡å‡†æ•°æ®é›†ä¸Šçš„æµ®ç‚¹æ•°å€¼åˆ†å¸ƒï¼Œå¯ä»¥æ›´å¥½åœ°é€‰æ‹©é‡åŒ–å‚æ•°ï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®ï¼Œæå‡é‡åŒ–åæ¨¡å‹çš„ç²¾åº¦ã€‚



Calibration è¿‡ç¨‹

1. **æ”¶é›†ç»Ÿè®¡ä¿¡æ¯**ï¼šå°†æ ¡å‡†æ•°æ®é›†è¾“å…¥æ¨¡å‹ï¼Œæ”¶é›†æ¯ä¸€å±‚çš„æ¿€æ´»å€¼å’Œæƒé‡çš„ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™äº›ç»Ÿè®¡ä¿¡æ¯åŒ…æ‹¬æœ€å°å€¼ã€æœ€å¤§å€¼ã€ç›´æ–¹å›¾åˆ†å¸ƒç­‰ã€‚
2. **è®¡ç®—é‡åŒ–å‚æ•°**ï¼šæ ¹æ®æ”¶é›†åˆ°çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè®¡ç®—é‡åŒ–æ‰€éœ€çš„ scale å’Œ zero-pointã€‚ä¾‹å¦‚ï¼Œå¦‚æœæŸå±‚çš„æ¿€æ´»å€¼èŒƒå›´æ˜¯ [a, b]ï¼Œåˆ™å¯ä»¥è®¡ç®—å‡ºé€‚åˆè¯¥èŒƒå›´çš„ scale å’Œ zero-pointã€‚
3. **åº”ç”¨é‡åŒ–å‚æ•°**ï¼šå°†è®¡ç®—å¾—åˆ°çš„é‡åŒ–å‚æ•°åº”ç”¨åˆ°æ¨¡å‹ä¸­ï¼Œå°†æµ®ç‚¹æ•°å€¼è½¬æ¢ä¸ºæ•´æ•°è¡¨ç¤ºã€‚



ä¸ºä»€ä¹ˆéœ€è¦ Calibration Dataset

- **çœŸå®æ•°æ®åˆ†å¸ƒ**ï¼šæ ¡å‡†æ•°æ®é›†åº”å°½å¯èƒ½åæ˜ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ä¼šé‡åˆ°çš„æ•°æ®åˆ†å¸ƒã€‚è¿™æ ·ç¡®å®šçš„é‡åŒ–å‚æ•°æ›´å…·æœ‰ä»£è¡¨æ€§ï¼Œé‡åŒ–åçš„æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ä¹Ÿä¼šæ›´å¥½ã€‚
- **é«˜æ•ˆå’Œå‡†ç¡®**ï¼šä½¿ç”¨æ ¡å‡†æ•°æ®é›†è¿›è¡Œç»Ÿè®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿã€å‡†ç¡®åœ°ç¡®å®šé‡åŒ–å‚æ•°ï¼Œå®ç°é«˜æ•ˆçš„é‡åŒ–è¿‡ç¨‹ã€‚



Calibration ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œ8ä½é‡åŒ–ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨æ ¡å‡†æ•°æ®é›†è¿›è¡Œ calibration çš„æ­¥éª¤ï¼š

```python
import numpy as np

# æ¨¡æ‹Ÿæ ¡å‡†æ•°æ®é›†
calibration_dataset = [np.random.randn(100, 224, 224, 3) for _ in range(10)]

# ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å‡½æ•°ï¼ˆç¤ºä¾‹ï¼‰
def collect_statistics(model, dataset):
    min_values = []
    max_values = []
    for data in dataset:
        activations = model(data)
        min_values.append(np.min(activations, axis=0))
        max_values.append(np.max(activations, axis=0))
    global_min = np.min(min_values, axis=0)
    global_max = np.max(max_values, axis=0)
    return global_min, global_max

# è®¡ç®—é‡åŒ–å‚æ•°
def calculate_quantization_params(global_min, global_max):
    scale = (global_max - global_min) / 255.0
    zero_point = -global_min / scale
    return scale, zero_point

# ç¤ºä¾‹æ¨¡å‹ï¼ˆå‡è®¾å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
class SimpleModel:
    def __call__(self, x):
        return x  # ç®€å•ä¼ é€’è¾“å…¥ä½œä¸ºè¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰

# æ¨¡å‹å®ä¾‹
model = SimpleModel()

# æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
global_min, global_max = collect_statistics(model, calibration_dataset)

# è®¡ç®—é‡åŒ–å‚æ•°
scale, zero_point = calculate_quantization_params(global_min, global_max)

print(f"Scale: {scale}, Zero Point: {zero_point}")
```



ç»“è®º

Post-Training Quantization (PTQ) çš„æ ¡å‡†ï¼ˆCalibrationï¼‰è¿‡ç¨‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒé€šè¿‡æ ¡å‡†æ•°æ®é›†æ¥ç»Ÿè®¡å’Œè®¡ç®—é‡åŒ–å‚æ•°ï¼Œç¡®ä¿åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å°½é‡å‡å°‘ç²¾åº¦æŸå¤±ã€‚Calibration æ•°æ®é›†æä¾›äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½é‡åˆ°çš„æ•°æ®ä¿¡æ¯ï¼Œå¸®åŠ©ç¡®å®šå‡†ç¡®çš„é‡åŒ–èŒƒå›´ï¼Œä»è€Œæé«˜é‡åŒ–æ¨¡å‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚





# Device Integration



![image-20240522150801288](./assets/image-20240522150801288.png)

How is the application implemented



![image-20240522150836998](./assets/image-20240522150836998.png)



Runtime dependencies

![image-20240522150927515](./assets/image-20240522150927515.png)



Demo

![image-20240522151128104](./assets/image-20240522151128104.png)



# Appendix - Building the App

Going into the details of the building the final mobile app was slightly outside the scope of this course. We have build this help guide for you to show the main steps and code samples you need to build the app we saw in the last lesson.



## TensorFlow Lite
TensorFlow Lite is a runtime that enables on-device machine learning by helping developers run their models on mobile, embedded, and edge devices. The models produced by TensorFlow Lite work on multiple platform support, covering Android and iOS devices, embedded Linux, and microcontrollers. The toolchain also has diverse language support, which includes Java, Swift, Objective-C, C++, and Python.

<a href="https://www.tensorflow.org/lite/guide">ğŸ”— &nbsp; TensorFlow Lite guide [ + ]</a>





## Delegation

In the context of TensorFlow Lite, "delegation" refers to the use of delegates to enable hardware acceleration of machine learning models on mobile and edge devices. Delegates act as a bridge between TensorFlow Lite and on-device accelerators like GPUs and DSPs, optimizing performance and efficiency by leveraging specialized hardware capabilities. This process can significantly improve the speed and power consumption of running machine learning models on such devices. For more details, you can visit the TensorFlow Lite Delegates page.

<a href="https://ai.google.dev/edge/lite/performance/delegates">ğŸ”— &nbsp; TensorFlow Lite Delegates page [ + ]</a>

### Qualcomm QNN delegate

Qualcomm QNN delegate allows you to run models on the NPU.

<a href="https://softwarecenter.qualcomm.com/api/download/software/qualcomm_neural_processing_sdk/v2.22.6.240515.zip">ğŸ”— &nbsp; Download Qualcomm QNN Delegate â€“ (Zip 724 MB)</a>





## End-to-end examples

You can find end-to-end examples, for common machine learning tasks such as image classification, object detection, pose estimation, question answering, text classification, etc. on multiple platforms. Here is the link to some models we have provided for you.

<a href="
https://github.com/quic/ai-hub-models/tree/main/apps/android">ğŸ”— &nbsp; End-to-end examples â€“ GitHub [ + ]</a>

<b>Models available</b>:
- ImageClassification
- ImageSuperResolution
- SemanticSegmentation





## Semantic segmentation code
The code for the semantic segmentation app we developed in this course is available on Github for you to try.

<b> Requirements:</b>
- Java, android-sdk and sdkmanager is already set at user's end
- User should have Linux QNN SDK in local machine.
- ANDROID_HOME is set to android-sdk path
- AI-Hub is properly configured with user token.

<b>Note:</b> Please execute build_apk.py. This script will compile and download a model from AI-Hub and paste it in your Android Project and Generate app-debug.apk.

<a href="https://github.com/quic/ai-hub-models/tree/main/apps/android/SemanticSegmentation">ğŸ”— &nbsp; Semantic segmentation code + Guide â€“ GitHub [ + ]</a>





# Afterword

2024å¹´5æœˆ22æ—¥äºä¸Šæµ·ã€‚

é€šè¿‡å­¦ä¹ è¿™é—¨short courseï¼Œäº†è§£äº†é«˜é€šå…¬å¸åœ¨on-device AIæ–¹é¢çš„åŠªåŠ›ï¼Œå¯¹AI Hub Modelsä»“åº“æœ‰äº†ä¸€å®šç¨‹åº¦çš„äº†è§£ã€‚